{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determinación variables en Matrícula\n",
    "###     >> por Jordi Palau, Enrique Rodríguez, Raúl Zafra <<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripción proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de librerías y de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos las librerías que utilizaremos en el desarrollo del proyecto:\n",
    "* pandas\n",
    "* numpy\n",
    "* datetime\n",
    "* dateutil\n",
    "* pylab\n",
    "* sklearn\n",
    "* __future__ \n",
    "\n",
    "Parametrizamos la ubicación de los archivos para facilitar la carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cargamos las librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from pylab import plot,show\n",
    "from numpy import vstack,array\n",
    "from numpy.random import rand\n",
    "#from scipy.cluster.vq import kmeans,vq\n",
    "from sklearn.cluster import KMeans\n",
    "from __future__ import division\n",
    "\n",
    "#Definimos variables de entorno\n",
    "\n",
    "# base_dir = r'D:\\Capstone-Wordreader\\Data'\n",
    "base_dir = r'/Data/'\n",
    "\n",
    "#cargamos fichero\n",
    "campanyes = pd.read_csv (os.path.join(base_dir,'dades-capstone-def3_2.csv'), parse_dates=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptivas de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la edad para disponer de una variable adicional con la que construir el modelo: [ Edad en años = (hoy - fecha de nacimiento)/365,24 ]\n",
    "\n",
    "Calculamos el número de intracciones por fecha y semestre para comparar la evolución de los intereses por día de campaña"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "campanyes.info()\n",
    "\n",
    "#validamos las columnas cargadas\n",
    "campanyes.columns\n",
    "\n",
    "# cálculo de la edad en años\n",
    "campanyes['data_naixement']=pd.to_datetime(campanyes['data_naixement'])\n",
    "campanyes['edad_dias']=(pd.to_datetime(campanyes['data_ini_lead'])-pd.to_datetime(campanyes['data_naixement']))\n",
    "campanyes['edad_anyos']=campanyes['edad_dias'].astype('timedelta64[D]')/365.24\n",
    "\n",
    "edad_media_anyos=campanyes['edad_anyos'].mean()\n",
    "edad_media_anyos\n",
    "\n",
    "campanyes['edad_anyos'].hist(bins=90) \n",
    "\n",
    "edad_media_anyos\n",
    "\n",
    "\n",
    "# recuento de usuarios por semestre\n",
    "campanyes.groupby(['semestre'])['identif_usuari'].count()\n",
    "\n",
    "# recuento de usuarios por \"estado de lead\" vs \"semestre\"\n",
    "campanyes.groupby(['semestre','estat_lead_recode'])['producte_comprat_recode'].count()\n",
    "\n",
    "# recuento de usuarios por \"matricula\" vs \"semestre\"\n",
    "campanyes.groupby(['semestre','matricula'])['producte_comprat_recode'].count()\n",
    "\n",
    "\n",
    "# evolución campañas 20151, 20152 y 20161\n",
    "campanya_20151 = campanyes[campanyes['semestre'] == 20151]\n",
    "campanya_20152 = campanyes[campanyes['semestre'] == 20152]\n",
    "campanya_20161 = campanyes[campanyes['semestre'] == 20161]\n",
    "campanya_20151\n",
    "campanya_20152\n",
    "campanya_20161\n",
    "\n",
    "campanya_20151['data_ini_lead_date']=pd.to_datetime(campanya_20151['data_ini_lead'])\n",
    "campanya_20152['data_ini_lead_date']=pd.to_datetime(campanya_20152['data_ini_lead'])\n",
    "campanya_20161['data_ini_lead_date']=pd.to_datetime(campanya_20161['data_ini_lead'])\n",
    "\n",
    "#graficamos la evolución de leads campanya 20151, 20152 y 20161 por fecha\n",
    "evol_campanya_20151=campanya_20151.groupby(['data_ini_lead_date'])['producte_comprat_recode'].count()\n",
    "evol_campanya_20152=campanya_20152.groupby(['data_ini_lead_date'])['producte_comprat_recode'].count()\n",
    "evol_campanya_20161=campanya_20161.groupby(['data_ini_lead_date'])['producte_comprat_recode'].count()\n",
    "\n",
    "evol_campanya_20151\n",
    "\n",
    "evol_campanya_20151.plot()\n",
    "evol_campanya_20152.plot()\n",
    "evol_campanya_20161.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción fichero que agrega variables a nivel de código de persona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos un fichero que agrega diversas métricas relacionadas con el perfil socio-demográfico de las personas interesada y las interacciones mantenidas con la organización:\n",
    "\n",
    "__Perfil sociodemográfico__\n",
    "* Sexo\n",
    "* Edad\n",
    "\n",
    "__Interacciones con la organización__\n",
    "* Producto\n",
    "* Punto de entrada\n",
    "* Area de producto\n",
    "* Subarea\n",
    "* Tipo de producto\n",
    "* Canal\n",
    "* Idioma\n",
    "* Semestre\n",
    "* Región\n",
    "\n",
    "__Matrícula__\n",
    "* El usuario se ha \"Matriculado\" / \"No se ha matriculado\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seleccionamos a los usuarios matriculados\n",
    "campanya_matriculats = campanyes[campanyes['just_lead_recode'] == 'Matriculado']\n",
    "campanya_matriculats = campanya_matriculats[['identif_usuari','just_lead_recode']] #seleccionamos la columna de matriculados\n",
    "campanya_matriculats \n",
    "\n",
    "# agrupamos por identif_usuari y calculamos first, promedio, count_distinct para diferentes campos\n",
    "Persones_Campanya_sexe = pd.DataFrame(campanyes.groupby(['identif_usuari'])['sexe'].first())  #sexo \n",
    "Persones_Campanya_edat = pd.DataFrame(campanyes.groupby(['identif_usuari'])['edad_anyos'].mean())  #edad\n",
    "Persones_Campanya_prodcomp = pd.DataFrame(campanyes.groupby(['identif_usuari'])['producte_comprat_recode'].nunique())  #producto comprado\n",
    "Persones_Campanya_puntentr = pd.DataFrame(campanyes.groupby(['identif_usuari'])['punt_entrada_recode'].nunique())  #punto entrada\n",
    "Persones_Campanya_area = pd.DataFrame(campanyes.groupby(['identif_usuari'])['area_prod_comprat_recode'].nunique())  #área\n",
    "Persones_Campanya_subarea = pd.DataFrame(campanyes.groupby(['identif_usuari'])['subarea_prod_comprat_recode'].nunique())  #subarea\n",
    "Persones_Campanya_tipusprod = pd.DataFrame(campanyes.groupby(['identif_usuari'])['tipus_producte_comprat'].nunique())  #tipo de producto\n",
    "Persones_Campanya_canal_recode = pd.DataFrame(campanyes.groupby(['identif_usuari'])['canal_recode'].nunique())  #canal\n",
    "Persones_Campanya_idioma_recode = pd.DataFrame(campanyes.groupby(['identif_usuari'])['idioma_recode'].nunique())  #idioma\n",
    "Persones_Campanya_semestre = pd.DataFrame(campanyes.groupby(['identif_usuari'])['semestre'].nunique())  #semestre\n",
    "Persones_Campanya_regio = pd.DataFrame(campanyes.groupby(['identif_usuari'])['regio'].nunique())  #región\n",
    "\n",
    "#añadimos índice para liberar el código de usuario que está siendo utilizado previamente como clave\n",
    "Persones_Campanya_sexe = Persones_Campanya_sexe.reset_index(drop=False)\n",
    "Persones_Campanya_edat = Persones_Campanya_edat.reset_index(drop=False)\n",
    "Persones_Campanya_prodcomp = Persones_Campanya_prodcomp.reset_index(drop=False)\n",
    "Persones_Campanya_puntentr = Persones_Campanya_puntentr.reset_index(drop=False)\n",
    "Persones_Campanya_area = Persones_Campanya_area.reset_index(drop=False)\n",
    "Persones_Campanya_subarea = Persones_Campanya_subarea.reset_index(drop=False)\n",
    "Persones_Campanya_tipusprod = Persones_Campanya_tipusprod.reset_index(drop=False)\n",
    "Persones_Campanya_canal_recode = Persones_Campanya_canal_recode.reset_index(drop=False)\n",
    "Persones_Campanya_idioma_recode = Persones_Campanya_idioma_recode.reset_index(drop=False)\n",
    "Persones_Campanya_semestre = Persones_Campanya_semestre.reset_index(drop=False)\n",
    "Persones_Campanya_regio = Persones_Campanya_regio.reset_index(drop=False)\n",
    "\n",
    "#unimos los dataframes con información por usuario a una tabla única\n",
    "Persones_activitat = pd.merge(Persones_Campanya_sexe, Persones_Campanya_edat, on='identif_usuari')\n",
    "Persones_activitat = pd.merge(Persones_activitat, Persones_Campanya_prodcomp, on='identif_usuari')\n",
    "Persones_activitat = pd.merge(Persones_activitat, Persones_Campanya_puntentr, on='identif_usuari')\n",
    "Persones_activitat = pd.merge(Persones_activitat, Persones_Campanya_area, on='identif_usuari')\n",
    "Persones_activitat = pd.merge(Persones_activitat, Persones_Campanya_subarea, on='identif_usuari')\n",
    "Persones_activitat = pd.merge(Persones_activitat, Persones_Campanya_tipusprod, on='identif_usuari')\n",
    "Persones_activitat = pd.merge(Persones_activitat, Persones_Campanya_canal_recode, on='identif_usuari')\n",
    "Persones_activitat = pd.merge(Persones_activitat, Persones_Campanya_idioma_recode, on='identif_usuari')\n",
    "Persones_activitat = pd.merge(Persones_activitat, Persones_Campanya_regio, on='identif_usuari')\n",
    "Persones_activitat = pd.merge(Persones_activitat, campanya_matriculats,how='left', on='identif_usuari')\n",
    "Persones_activitat['just_lead_recode'].fillna('No Matriculado', inplace=True)\n",
    "Persones_activitat = Persones_activitat.rename(columns={'just_lead_recode':'estado_matricula'})\n",
    "Persones_activitat.groupby('estado_matricula')['identif_usuari'].count()\n",
    "\n",
    "Persones_activitat.describe() #descriptivas de los índices obtenidos\n",
    "\n",
    "#Borramos las tablas intermedias\n",
    "del Persones_Campanya_sexe\n",
    "del Persones_Campanya_edat\n",
    "del Persones_Campanya_prodcomp\n",
    "del Persones_Campanya_puntentr\n",
    "del Persones_Campanya_area\n",
    "del Persones_Campanya_subarea\n",
    "del Persones_Campanya_tipusprod\n",
    "del Persones_Campanya_canal_recode\n",
    "del Persones_Campanya_idioma_recode\n",
    "del Persones_Campanya_semestre\n",
    "del Persones_Campanya_regio\n",
    "del campanya_matriculats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agrupación mediante K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los objetivos de nuestra investigación es segmentar la base de datos en n grupos homogéneos (para n = 2,3,4, ...) de modo que los miembros de cada uno de ellos tengan características lo más parecidas posible entre ellos y sean lo más diferente posible respecto al resto de grupos.\n",
    "\n",
    "Para segmentar la base de datos utilizaremos el método del K-means aplicado a las variables relacionadas con la personalidad de los estudiantes (pregunta 29 del cuestionario) como variable activa para, una vez obtenidos los segmentos, comparar los perfiles de los grupos obtenidos.\n",
    "\n",
    "Una vez escogidas las variables, aplicamos los siguientes pasos para buscar la solución que nos permita obtener una mejor agrupación de los miembros del archivo:\n",
    "\n",
    "* Obtener las soluciones de aplicar el método K-means por un número de agrupaciones comprendido entre 3 y 8 (es decir, 3, 4, 5 ... hasta 8 agrupaciones).\n",
    "* Comparar el tamaño, el perfil y la estabilidad de las soluciones obtenidas; es decir, evaluar el tamaño, el perfil y el comportamiento de los grupos a medida que disgregan / agregamos los individuos en un número diferente de clústeres.\n",
    "* Elegir la mejor solución: perfiles diferenciados y tamaños representativos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_clustering = Persones_activitat.ix[:,3:11]\n",
    "data_clustering\n",
    "\n",
    "kmeans_n3 = KMeans(n_clusters=3)\n",
    "kmeans_n3.fit(data_clustering)\n",
    "kmeans_n3\n",
    "centroid_n3 = kmeans_n3.cluster_centers_\n",
    "cluster_n3 = kmeans_n3.labels_\n",
    "\n",
    "kmeans_n4 = KMeans(n_clusters=4)\n",
    "kmeans_n4.fit(data_clustering)\n",
    "kmeans_n4\n",
    "centroid_n4 = kmeans_n4.cluster_centers_\n",
    "cluster_n4 = kmeans_n4.labels_\n",
    "\n",
    "kmeans_n5 = KMeans(n_clusters=5)\n",
    "kmeans_n5.fit(data_clustering)\n",
    "kmeans_n5\n",
    "centroid_n5 = kmeans_n5.cluster_centers_\n",
    "cluster_n5 = kmeans_n5.labels_\n",
    "\n",
    "kmeans_n6 = KMeans(n_clusters=6)\n",
    "kmeans_n6.fit(data_clustering)\n",
    "kmeans_n6\n",
    "centroid_n6 = kmeans_n6.cluster_centers_\n",
    "cluster_n6 = kmeans_n6.labels_\n",
    "\n",
    "kmeans_n7 = KMeans(n_clusters=7)\n",
    "kmeans_n7.fit(data_clustering)\n",
    "kmeans_n7\n",
    "centroid_n7 = kmeans_n7.cluster_centers_\n",
    "cluster_n7 = kmeans_n7.labels_\n",
    "\n",
    "kmeans_n8 = KMeans(n_clusters=8)\n",
    "kmeans_n8.fit(data_clustering)\n",
    "kmeans_n8\n",
    "centroid_n8 = kmeans_n8.cluster_centers_\n",
    "cluster_n8 = kmeans_n8.labels_\n",
    "\n",
    "\n",
    "print (centroid_n3)\n",
    "print (centroid_n4)\n",
    "print (centroid_n5)\n",
    "print (centroid_n6)\n",
    "print (centroid_n7)\n",
    "print (centroid_n8)\n",
    "\n",
    "cluster_n3=pd.DataFrame(cluster_n3)\n",
    "cluster_n4=pd.DataFrame(cluster_n4)\n",
    "cluster_n5=pd.DataFrame(cluster_n5)\n",
    "cluster_n6=pd.DataFrame(cluster_n6)\n",
    "cluster_n7=pd.DataFrame(cluster_n7)\n",
    "cluster_n8=pd.DataFrame(cluster_n8)\n",
    "\n",
    "cluster_n3 = cluster_n3.rename(columns={0:'cluster_n3'})\n",
    "cluster_n4 = cluster_n4.rename(columns={0:'cluster_n4'})\n",
    "cluster_n5 = cluster_n5.rename(columns={0:'cluster_n5'})\n",
    "cluster_n6 = cluster_n6.rename(columns={0:'cluster_n6'})\n",
    "cluster_n7 = cluster_n7.rename(columns={0:'cluster_n7'})\n",
    "cluster_n8 = cluster_n8.rename(columns={0:'cluster_n8'})\n",
    "\n",
    "frame=[Persones_activitat,cluster_n3,cluster_n4,cluster_n5,cluster_n6,cluster_n7,cluster_n8]\n",
    "Persones_activitat_cluster=pd.concat(frame,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación mediante KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es un método de clasificación no paramétrico, que estima el valor de la probabilidad a posteriori de que un elemento x pertenezca a una clase en particular a partir de la información proporcionada por el conjunto de prototipos. La regresión KNN se calcula simplemente tomando el promedio del punto k más cercano al punto que se está probando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Isolate target data\n",
    "churn_result = Persones_activitat['estado_matricula']\n",
    "y = np.where(churn_result == 'Matriculado',1,0)\n",
    "\n",
    "# We don't need these columns\n",
    "to_drop = ['identif_usuari','sexe','edad_anyos','estado_matricula']\n",
    "churn_feat_space = Persones_activitat.drop(to_drop,axis=1)\n",
    "\n",
    "# 'yes'/'no' has to be converted to boolean values\n",
    "# NumPy converts these from boolean to 1. and 0. later\n",
    "#yes_no_cols = [\"Int'l Plan\",\"VMail Plan\"]\n",
    "#churn_feat_space[yes_no_cols] = churn_feat_space[yes_no_cols] == 'yes'\n",
    "\n",
    "# Pull out features for future use\n",
    "features = churn_feat_space.columns\n",
    "\n",
    "X = churn_feat_space.as_matrix().astype(np.float)\n",
    "\n",
    "print \"Feature space holds %d observations and %d features\" % X.shape\n",
    "print \"Unique target labels:\", np.unique(y)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.pie(np.c_[len(y)-np.sum(y),np.sum(y)][0],labels=['No Churn','Churn'],colors=['r','g'],shadow=True,autopct ='%.2f' )\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(6,6)\n",
    "\n",
    "import pickle\n",
    "ofname = open('churn_data.pkl', 'wb')\n",
    "s = pickle.dump([X,y,features],ofname)\n",
    "ofname.close()\n",
    "\n",
    "\n",
    "\n",
    "#Let's see what the boundary looks like in a toy problem.\n",
    "\n",
    "MAXN=10\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "idxplus = y==1\n",
    "idxminus = y==-1\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "\n",
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 15.0, delta)\n",
    "yy = np.arange(-5.0, 15.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.c_[Xf[:,np.newaxis],Yf[:,np.newaxis]];\n",
    "\n",
    "\n",
    "#Evaluate the model for a given weight\n",
    "clf = neighbors.KNeighborsClassifier(1)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "\n",
    "\n",
    "#Let's see what the boundary looks like in a toy problem.\n",
    "\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(3)\n",
    "clf.fit(X,y.ravel())\n",
    "Z2=clf.predict(data)\n",
    "Z2.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.4, vmin=-1, vmax=1)\n",
    "plt.imshow(Z2, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.2, vmin=-1, vmax=1)\n",
    "\n",
    "plt.contour(XX,YY,Z2,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "\n",
    "\n",
    "#Recover Churn data\n",
    "import pickle\n",
    "fname = open('churn_data.pkl','rb')\n",
    "data = pickle.load(fname)\n",
    "X = data[0]\n",
    "y = data[1]\n",
    "print 'Loading ok.'\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "kf=cross_validation.KFold(n=y.shape[0], n_folds=5, shuffle=False, random_state=0)\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf:\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dt = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "    dt.fit(X_train,y_train)\n",
    "    yhat[test_index] = dt.predict(X_test)\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print 'Mean accuracy: '+ str(np.mean(acc))\n",
    "\n",
    "\n",
    "\n",
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in xrange(2):\n",
    "        for j in xrange(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print metrics.classification_report(y,yhat)\n",
    "\n",
    "\n",
    "\n",
    "# Standarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "acc_snooping = np.zeros((5,))\n",
    "i=0\n",
    "kf=cross_validation.KFold(n=y.shape[0], n_folds=5, shuffle=False, random_state=0)\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf:\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dt = neighbors.KNeighborsClassifier(3)\n",
    "    dt.fit(X_train,y_train)\n",
    "    yhat[test_index] = dt.predict(X_test)\n",
    "    acc_snooping[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print 'Mean accuracy: '+ str(np.mean(acc_snooping))\n",
    "\n",
    "\n",
    "\n",
    "#NO SNOOPING\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf:\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    dt = neighbors.KNeighborsClassifier(3)\n",
    "    dt.fit(X_train,y_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    yhat[test_index] = dt.predict(X_test)\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print 'Mean accuracy: '+ str(np.mean(acc))\n",
    "\n",
    "\n",
    "acct=np.c_[acc_snooping,acc]\n",
    "plt.boxplot(acct);\n",
    "for i in xrange(2):\n",
    "    xderiv = (i+1)*np.ones(acct[:,i].shape)+(np.random.rand(5,)-0.5)*0.1\n",
    "    plt.plot(xderiv,acct[:,i],'ro',alpha=0.3)\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(['snooping', 'no snooping'])\n",
    "\n",
    "\n",
    "\n",
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in xrange(2):\n",
    "        for j in xrange(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print metrics.classification_report(y,yhat)\n",
    "\n",
    "\n",
    "\n",
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    ax.text(0, 0, 'TP', va='center', ha='center',color='white',size=20)\n",
    "    ax.text(0, 1, 'FN', va='center', ha='center',color='white',size=20)\n",
    "    ax.text(1, 0, 'FP', va='center', ha='center',color='white',size=20)\n",
    "    ax.text(1, 1, 'TN', va='center', ha='center',color='white',size=20)            \n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['positive', 'negative'])\n",
    "\n",
    "\n",
    "\n",
    "# Let us check the concepts with churn as the positive class\n",
    "TP = np.sum(np.logical_and(yhat==1,y==1))\n",
    "TN = np.sum(np.logical_and(yhat==0,y==0))\n",
    "FP = np.sum(np.logical_and(yhat==1,y==0))\n",
    "FN = np.sum(np.logical_and(yhat==0,y==1))\n",
    "\n",
    "print 'TP: ' + str(TP)\n",
    "print 'TN: ' + str(TN)\n",
    "print 'FP: ' + str(FP)\n",
    "print 'FN: ' + str(FN)\n",
    "print 'sensitivity/recall: '+ str(TP/(TP+FN))\n",
    "print 'precision: '+ str(TP/(TP+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación mediante Árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Arboles de Decision son diagramas con construcciones lógicas, muy similares a los sistemas de predicción basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resolución de un problema. Los Arboles de Decision están compuestos por nodos interiores, nodos terminales y ramas que emanan de los nodos interiores. Cada nodo interior en el árbol contiene una prueba de un atributo, y cada rama representa un valor distinto del atributo. Siguiendo las ramas desde el nodo raíz hacia abajo, cada ruta finalmente termina en un nodo terminal creando una segmentación de los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "#Let's see what the boundary looks like in a toy problem.\n",
    "%reset -f\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "MAXN=10\n",
    "np.random.seed(2)\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "idxplus = y==1\n",
    "idxminus = y==-1\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 15.0, delta)\n",
    "yy = np.arange(-5.0, 15.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.c_[Xf[:,np.newaxis],Yf[:,np.newaxis]];\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "\n",
    "#Export Tree\n",
    "import os\n",
    "dotfile = tree.export_graphviz(clf, out_file = \"toy_tree.dot\")\n",
    "\n",
    "os.system(\"dot -Tpng toy_tree.dot -o toy_tree.png\")\n",
    "\n",
    "from IPython.core.display import Image\n",
    "Image(\"toy_tree.png\")\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(random_state=0,max_depth=1)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "\n",
    "\n",
    "\n",
    "entropy = lambda p: -np.sum(p * np.log2(p)) if not 0 in p else 0\n",
    "gini = lambda p: 1. - (np.array(p)**2).sum()\n",
    "pvals = np.linspace(0, 1)        \n",
    "plt.plot(pvals, [entropy([p,1-p])/2. for p in pvals], label='Entropy')\n",
    "plt.plot(pvals, [gini([p,1-p]) for p in pvals], label='Gini')\n",
    "plt.legend()\n",
    "\n",
    "%reset -f\n",
    "#Recover Churn data\n",
    "import pickle\n",
    "fname = open('churn_data.pkl','rb')\n",
    "data = pickle.load(fname)\n",
    "X = data[0]\n",
    "y = data[1]\n",
    "features = data[2]\n",
    "print 'Loading ok.'\n",
    "\n",
    "#NO SNOOPING\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "kf=cross_validation.KFold(n=y.shape[0], n_folds=5, shuffle=False, random_state=0)\n",
    "\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf:\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "    dt.fit(X_train,y_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    yhat[test_index] = dt.predict(X_test)\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print 'Mean accuracy: '+ str(np.mean(acc))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in xrange(2):\n",
    "        for j in xrange(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print metrics.classification_report(y,yhat)\n",
    "\n",
    "# Let us check the concepts with churn as the positive class\n",
    "TP = np.sum(np.logical_and(yhat==1,y==1))\n",
    "TN = np.sum(np.logical_and(yhat==0,y==0))\n",
    "FP = np.sum(np.logical_and(yhat==1,y==0))\n",
    "FN = np.sum(np.logical_and(yhat==0,y==1))\n",
    "\n",
    "print 'TP: ' + str(TP)\n",
    "print 'TN: ' + str(TN)\n",
    "print 'FP: ' + str(FP)\n",
    "print 'FN: ' + str(FN)\n",
    "print 'sensitivity/recall: '+ str(TP/(TP+FN))\n",
    "print 'precision: '+ str(TP/(TP+FP))\n",
    "\n",
    "import os\n",
    "#Let us check the the first three levels of the tree. GraphViz and PyDot are needed.\n",
    "dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "dt.fit(Xs,y)\n",
    "\n",
    "#Export Tree\n",
    "\n",
    "dotfile = tree.export_graphviz(dt, out_file = \"churn.dot\", feature_names = features)\n",
    "\n",
    "os.system(\"dot -Tpng churn.dot -o churn.png\")\n",
    "\n",
    "from IPython.core.display import Image\n",
    "Image(\"churn.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
